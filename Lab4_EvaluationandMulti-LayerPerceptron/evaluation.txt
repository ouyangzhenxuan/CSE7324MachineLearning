To validate our model, there are several methods we can choose.           
The basic method is hold-out method. It just separates the dataset into the training and the testing set randomly. Although this method is easy to implement, it has a weakness which is if the training or the testing set is unrepresentative, the model can be skewed. So, this method is not appropriate.                    
The next one is k-fold cross validation. In CV, we break the data up into K partitions and then, K times in turn, we select one partition for testing and use the remaining ones for training. It solve the problem of the first method. However, in this lab, we need to use cross validation for several times to implement the grid search. Obviously, it is not sufficient. We need a three-way split.        
The last but not the least one is the nested cross validation. The dataset will be splitted into 3 datasets: the training set, the testing set and the validation set. The inner cv is the training/validation partition, and the outer cv is the validation/testing partition. So, using this method, we can run the program just once to test all parameters and find the winning model.    
According to the above, we are going to use the nested cross validation with the grid search.
