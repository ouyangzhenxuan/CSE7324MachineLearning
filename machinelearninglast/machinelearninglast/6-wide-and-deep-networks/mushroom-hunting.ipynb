{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom Hunting\n",
    "Lab Assignment One: Exploring Table Data\n",
    "\n",
    "**_Jake Oien, Seung Ki Lee, Jenn Le_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data can be useful in identifying trends in poisonous mushrooms and assist in the classification of unknown mushrooms.\n",
    "\n",
    "From the dataset's description: \n",
    "\"This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for Poisonous Oak and Ivy.\"\n",
    "\n",
    "According to http://www.amjbot.org/content/98/3/426.full there are estimated to be upwards of 5 million species of fungi, with only around 100,000 of them having been discovered and documented. At estimated discovery rates of 1200 species per year, scientists estimate it could take as long as 4000 years to discover all species of fungi. \n",
    "\n",
    "What this means is that only the tip of the iceberg has been studied as far as fungi is concerned. The dataset mentions that \"shrooming\" is experiencing a boom in popularity. With such a low percentage of documented fungi, it's possible that someone walking through a forest could happen upon a species of mushroom that's never been seen. People are curious, and someone is bound to want to try eating this strange mushroom. \n",
    "\n",
    "The end goal of analysing a dataset like this would be to classify an unknown specimen of mushroom as edible or poisonous. As the Audobon Society Field Guide says, \"there is no simple rule for determining the edibility of a mushroom.\" This might be true. But, there may be some underlying pattern between a collection of variables in a mushroom that might provide a (slightly more complex) rule for if a mushroom is poisonous or not. This may not be a \"sexy\" avenue of research for machine learning, but a successful classification algorithm could open a door into further research to help better understand broader categories of fungi, and better understanding about the life with which we share the planet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure of success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to discuss what a successful algorithm actually means, or, how do we determine if a machine learning algorithm is a success on this dataset. \n",
    "\n",
    "If the algorithm is learning properly, we should expect the success rate to be better than random chance (in this case 50%). Obviously, we should not expect to achieve a 100% success rate, but we should strive for as close as possible. There are two main \"users\" that could benefit from a successful classification, one being a scientist and the other being a potential consumer of unknown mushrooms. \n",
    "\n",
    "A scientist might look at the data and say that a 90% true positive rate on classification is \"good enough\" for further research, and digging into why certain classifications failed or succeeded might provide more insight into mushrooms as a whole. A success rate around that range would show that the algorithm is sound, and perhaps just needs some fine tuning or more data to achieve a higher success rate. \n",
    "\n",
    "However, the consumer of unknown mushrooms should not be content with a 90% true positive rate. False negatives would not be of a concern. They're not missing out on much except a new experience, after all. However, a 10% false positive rate would mean that 1 out of 10 times, an app that tells you \"This mushroom is safe to eat\" would be wrong and you would die. In order to be used on a consumer level, a very low false positive rate would be required to be remotely acceptable. To answer the question of if the machine learning algorithm is acceptable for use in aiding a \"mushroom enthusiast,\" we should look at other levels of risk that are accepted in similar situations. \n",
    "\n",
    "Fugu (pufferfish) is dish that if prepared improperly, can cause at the least severe poisoning and at worst death. Despite that, people still eat the dish, because they trust that the chef who prepares it knows what they are doing.  According to http://www.fukushihoken.metro.tokyo.jp/shokuhin/hugu/, 354 people in Japan suffered adverse effects from eating poorly prepared fugu over the last 10 years. The statistic for how many people ate fugu in that time is not known, but we can come up with a skeptical number to compare that adverse effect rate to the false positive rate of the algorithm. According to http://www.worldometers.info/world-population/japan-population/, Japan's current population is just under 126 million people. Of course, everyone in Japan did not eat fugu in that time. So if we use a conservative estimate that 1% of the population ate fugu one time during that 10 year period we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated rate of adverse effects due to fugu consumption: 0.0002832\n"
     ]
    }
   ],
   "source": [
    "adverse_effects = 354\n",
    "japanese_population = 125000000\n",
    "total_fugu_consumption_estimate = japanese_population/100 # 1% of the population ate fugu one time\n",
    "\n",
    "print(\"Estimated rate of adverse effects due to fugu consumption: {}\"\n",
    "      .format(adverse_effects/total_fugu_consumption_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one means by which an algorithm should be held to a false positive rate. This is just an estimate, because without per capita consumption estimates, a best guess is all we can hope for. However, at the bare minimum, someone should be more confident in the algorithm telling them that a random mushroom is safe to eat than they should be that their highly trained chef in a controlled environment prepared their deadly dish properly. Thus, the requirements for success for human consumption are (obviously) quite high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Source: https://www.kaggle.com/uciml/mushroom-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
       "0     p         x           s         n       t    p               f   \n",
       "1     e         x           s         y       t    a               f   \n",
       "2     e         b           s         w       t    l               f   \n",
       "3     p         x           y         w       t    p               f   \n",
       "4     e         x           s         g       f    n               f   \n",
       "\n",
       "  gill-spacing gill-size gill-color   ...   stalk-surface-below-ring  \\\n",
       "0            c         n          k   ...                          s   \n",
       "1            c         b          k   ...                          s   \n",
       "2            c         b          n   ...                          s   \n",
       "3            c         n          n   ...                          s   \n",
       "4            w         b          k   ...                          s   \n",
       "\n",
       "  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
       "0                      w                      w         p          w   \n",
       "1                      w                      w         p          w   \n",
       "2                      w                      w         p          w   \n",
       "3                      w                      w         p          w   \n",
       "4                      w                      w         p          w   \n",
       "\n",
       "  ring-number ring-type spore-print-color population habitat  \n",
       "0           o         p                 k          s       u  \n",
       "1           o         p                 n          n       g  \n",
       "2           o         p                 n          n       m  \n",
       "3           o         p                 k          s       u  \n",
       "4           o         e                 n          a       g  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./mushrooms.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Value Replacements\n",
    "\n",
    "We're unable to get a picture for what this dataset actually is because we don't know what the labels mean. In order to make it more intuitive, we replace the labels with more meaningful values, descriptive labels or variable representations where it makes sense.\n",
    "\n",
    "Just to provide a clearer picture of the dataset, we will change the labels to their full-word definition. Later, we will one-hot encode most of the dataset and scale the rest as appropriate to prepare it for use in Tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap_shape</th>\n",
       "      <th>cap_surface</th>\n",
       "      <th>cap_color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill_attachment</th>\n",
       "      <th>gill_spacing</th>\n",
       "      <th>gill_size</th>\n",
       "      <th>gill_color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk_surface_below_ring</th>\n",
       "      <th>stalk_color_above_ring</th>\n",
       "      <th>stalk_color_below_ring</th>\n",
       "      <th>veil_type</th>\n",
       "      <th>veil_color</th>\n",
       "      <th>ring_number</th>\n",
       "      <th>ring_type</th>\n",
       "      <th>spore_print_color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poisonous</td>\n",
       "      <td>convex</td>\n",
       "      <td>smooth</td>\n",
       "      <td>brown</td>\n",
       "      <td>1</td>\n",
       "      <td>pungent</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>narrow</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pendant</td>\n",
       "      <td>black</td>\n",
       "      <td>scattered</td>\n",
       "      <td>urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-poisonous</td>\n",
       "      <td>convex</td>\n",
       "      <td>smooth</td>\n",
       "      <td>yellow</td>\n",
       "      <td>1</td>\n",
       "      <td>almond</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>broad</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pendant</td>\n",
       "      <td>brown</td>\n",
       "      <td>numerous</td>\n",
       "      <td>grasses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-poisonous</td>\n",
       "      <td>bell</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>anise</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>broad</td>\n",
       "      <td>brown</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pendant</td>\n",
       "      <td>brown</td>\n",
       "      <td>numerous</td>\n",
       "      <td>meadows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poisonous</td>\n",
       "      <td>convex</td>\n",
       "      <td>scaly</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pungent</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>narrow</td>\n",
       "      <td>brown</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pendant</td>\n",
       "      <td>black</td>\n",
       "      <td>scattered</td>\n",
       "      <td>urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-poisonous</td>\n",
       "      <td>convex</td>\n",
       "      <td>smooth</td>\n",
       "      <td>gray</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>free</td>\n",
       "      <td>crowded</td>\n",
       "      <td>broad</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>evanescent</td>\n",
       "      <td>brown</td>\n",
       "      <td>abundant</td>\n",
       "      <td>grasses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>non-poisonous</td>\n",
       "      <td>convex</td>\n",
       "      <td>scaly</td>\n",
       "      <td>yellow</td>\n",
       "      <td>1</td>\n",
       "      <td>almond</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>broad</td>\n",
       "      <td>brown</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pendant</td>\n",
       "      <td>black</td>\n",
       "      <td>numerous</td>\n",
       "      <td>grasses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>non-poisonous</td>\n",
       "      <td>bell</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>almond</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>broad</td>\n",
       "      <td>gray</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pendant</td>\n",
       "      <td>black</td>\n",
       "      <td>numerous</td>\n",
       "      <td>meadows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>non-poisonous</td>\n",
       "      <td>bell</td>\n",
       "      <td>scaly</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>anise</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>broad</td>\n",
       "      <td>brown</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pendant</td>\n",
       "      <td>brown</td>\n",
       "      <td>scattered</td>\n",
       "      <td>meadows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poisonous</td>\n",
       "      <td>convex</td>\n",
       "      <td>scaly</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pungent</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>narrow</td>\n",
       "      <td>pink</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pendant</td>\n",
       "      <td>black</td>\n",
       "      <td>several</td>\n",
       "      <td>grasses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>non-poisonous</td>\n",
       "      <td>bell</td>\n",
       "      <td>smooth</td>\n",
       "      <td>yellow</td>\n",
       "      <td>1</td>\n",
       "      <td>almond</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>broad</td>\n",
       "      <td>gray</td>\n",
       "      <td>...</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>partial</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>pendant</td>\n",
       "      <td>black</td>\n",
       "      <td>scattered</td>\n",
       "      <td>meadows</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           class cap_shape cap_surface cap_color  bruises     odor  \\\n",
       "0      poisonous    convex      smooth     brown        1  pungent   \n",
       "1  non-poisonous    convex      smooth    yellow        1   almond   \n",
       "2  non-poisonous      bell      smooth     white        1    anise   \n",
       "3      poisonous    convex       scaly     white        1  pungent   \n",
       "4  non-poisonous    convex      smooth      gray        0     none   \n",
       "5  non-poisonous    convex       scaly    yellow        1   almond   \n",
       "6  non-poisonous      bell      smooth     white        1   almond   \n",
       "7  non-poisonous      bell       scaly     white        1    anise   \n",
       "8      poisonous    convex       scaly     white        1  pungent   \n",
       "9  non-poisonous      bell      smooth    yellow        1   almond   \n",
       "\n",
       "  gill_attachment gill_spacing gill_size gill_color   ...     \\\n",
       "0            free        close    narrow      black   ...      \n",
       "1            free        close     broad      black   ...      \n",
       "2            free        close     broad      brown   ...      \n",
       "3            free        close    narrow      brown   ...      \n",
       "4            free      crowded     broad      black   ...      \n",
       "5            free        close     broad      brown   ...      \n",
       "6            free        close     broad       gray   ...      \n",
       "7            free        close     broad      brown   ...      \n",
       "8            free        close    narrow       pink   ...      \n",
       "9            free        close     broad       gray   ...      \n",
       "\n",
       "  stalk_surface_below_ring stalk_color_above_ring stalk_color_below_ring  \\\n",
       "0                   smooth                  white                  white   \n",
       "1                   smooth                  white                  white   \n",
       "2                   smooth                  white                  white   \n",
       "3                   smooth                  white                  white   \n",
       "4                   smooth                  white                  white   \n",
       "5                   smooth                  white                  white   \n",
       "6                   smooth                  white                  white   \n",
       "7                   smooth                  white                  white   \n",
       "8                   smooth                  white                  white   \n",
       "9                   smooth                  white                  white   \n",
       "\n",
       "  veil_type veil_color ring_number   ring_type spore_print_color  population  \\\n",
       "0   partial      white           1     pendant             black   scattered   \n",
       "1   partial      white           1     pendant             brown    numerous   \n",
       "2   partial      white           1     pendant             brown    numerous   \n",
       "3   partial      white           1     pendant             black   scattered   \n",
       "4   partial      white           1  evanescent             brown    abundant   \n",
       "5   partial      white           1     pendant             black    numerous   \n",
       "6   partial      white           1     pendant             black    numerous   \n",
       "7   partial      white           1     pendant             brown   scattered   \n",
       "8   partial      white           1     pendant             black     several   \n",
       "9   partial      white           1     pendant             black   scattered   \n",
       "\n",
       "   habitat  \n",
       "0    urban  \n",
       "1  grasses  \n",
       "2  meadows  \n",
       "3    urban  \n",
       "4  grasses  \n",
       "5  grasses  \n",
       "6  meadows  \n",
       "7  meadows  \n",
       "8  grasses  \n",
       "9  meadows  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing column names\n",
    "for col_name, col in df.iteritems():\n",
    "    df.rename(columns={col_name:col_name.replace('-', '_')}, inplace=True)\n",
    "\n",
    "# Replacing data values, also shows the possible values for each attribute\n",
    "df[\"class\"].replace(to_replace=['e', 'p'],\n",
    "                     value=[\"non-poisonous\", \"poisonous\"], inplace=True)\n",
    "\n",
    "df.cap_shape.replace(to_replace=['b', 'c', 'x', 'f', 'k', 's'],\n",
    "                     value=[\"bell\", \"conical\", \"convex\", \"flat\", \"knobbed\", \"sunken\"],\n",
    "                     inplace=True)\n",
    "\n",
    "df.cap_surface.replace(to_replace=['f', 'g', 'y', 's'],\n",
    "                     value=[\"fibrous\", \"grooves\", \"scaly\", \"smooth\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.cap_color.replace(to_replace=['n', 'b', 'c', 'g', 'r', 'p', 'u', 'e', 'w', 'y'],\n",
    "                     value=[\"brown\", \"buff\", \"cinnamon\", \"gray\", \"green\", \"pink\", \n",
    "                            \"purple\", \"red\", \"white\", \"yellow\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.bruises.replace(to_replace=['f', 't'],\n",
    "                     value=[0, 1], \n",
    "                     inplace=True)\n",
    "\n",
    "df.odor.replace(to_replace=['a', 'l', 'c', 'y', 'f', 'm', 'n', 'p', 's'],\n",
    "                     value=[\"almond\", \"anise\", \"creosote\", \"fishy\", \"foul\", \n",
    "                            \"musty\", \"none\", \"pungent\", \"spicy\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.gill_attachment.replace(to_replace=['a', 'd', 'f', 'n'],\n",
    "                     value=[\"attached\", \"descending\", \"free\", \"notched\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.gill_spacing.replace(to_replace=['c', 'w', 'd'],\n",
    "                     value=[\"close\", \"crowded\", \"distant\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.gill_size.replace(to_replace=['b', 'n'],\n",
    "                     value=[\"broad\", \"narrow\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.gill_color.replace(to_replace=['k', 'n', 'b', 'h', 'g', 'r', 'o', \n",
    "                                  'p', 'u', 'e', 'w', 'y'],\n",
    "                     value=[\"black\", \"brown\", \"buff\", \"chocolate\", \"gray\", \n",
    "                            \"green\", \"orange\", \"pink\", \"purple\", \"red\", \"white\", \"yellow\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.stalk_shape.replace(to_replace=['e', 't'],\n",
    "                     value=[\"enlarging\", \"tapering\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.stalk_root.replace(to_replace=['b', 'c', 'u', 'e', 'z', 'r', '?'],\n",
    "                     value=[\"bulbous\", \"club\", \"cup\", \"equal\", \"rhizomorphs\", \n",
    "                            \"rooted\", \"missing\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.stalk_surface_above_ring.replace(to_replace=['f', 'y', 'k', 's'],\n",
    "                     value=[\"fibrous\", \"scaly\", \"silky\", \"smooth\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.stalk_surface_below_ring.replace(to_replace=['f', 'y', 'k', 's'],\n",
    "                     value=[\"fibrous\", \"scaly\", \"silky\", \"smooth\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.stalk_color_above_ring.replace(to_replace=['n', 'b', 'c', 'g', 'o', 'p', 'e', 'w', 'y'],\n",
    "                     value=[\"brown\", \"buff\", \"cinnamon\", \"gray\", \"orange\", \n",
    "                            \"pink\", \"red\", \"white\", \"yellow\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.stalk_color_below_ring.replace(to_replace=['n', 'b', 'c', 'g', 'o', 'p', 'e', 'w', 'y'],\n",
    "                     value=[\"brown\", \"buff\", \"cinnamon\", \"gray\", \"orange\", \n",
    "                            \"pink\", \"red\", \"white\", \"yellow\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.veil_type.replace(to_replace=['p', 'u'],\n",
    "                     value=[\"partial\", \"universal\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.veil_color.replace(to_replace=['n', 'o', 'w', 'y'],\n",
    "                     value=[\"brown\", \"orange\", \"white\", \"yellow\"], inplace=True)\n",
    "\n",
    "df.ring_number.replace(to_replace=['n', 'o', 't'],\n",
    "                     value=[0, 1, 2], inplace=True)\n",
    "\n",
    "df.ring_type.replace(to_replace=['c', 'e', 'f', 'l', 'n', 'p', 's', 'z'],\n",
    "                     value=[\"cobwebby\", \"evanescent\", \"flaring\", \"large\", \n",
    "                            \"none\", \"pendant\", \"sheathing\", \"zone\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.spore_print_color.replace(to_replace=['k', 'n', 'b', 'h', 'r', 'o', 'u', 'w', 'y'],\n",
    "                     value=[\"black\", \"brown\", \"buff\", \"chocolate\", \"green\", \n",
    "                            \"orange\", \"purple\", \"white\", \"yellow\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.population.replace(to_replace=['a', 'c', 'n', 's', 'v', 'y'],\n",
    "                     value=[\"abundant\", \"clustered\", \"numerous\", \"scattered\", \n",
    "                            \"several\", \"solitary\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.habitat.replace(to_replace=['g', 'l', 'm', 'p', 'u', 'w', 'd'],\n",
    "                     value=[\"grasses\", \"leaves\", \"meadows\", \"paths\", \"urban\", \n",
    "                            \"waste\", \"woods\"], \n",
    "                     inplace=True)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.202855736090591"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of possibly poisonous mushrooms in the dataset\n",
    "sum(df[\"class\"]==\"poisonous\")/len(df)*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things we look for to determine data quality; missing values and duplicate data. First, we'll look for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8124 entries, 0 to 8123\n",
      "Data columns (total 23 columns):\n",
      "class                       8124 non-null object\n",
      "cap_shape                   8124 non-null object\n",
      "cap_surface                 8124 non-null object\n",
      "cap_color                   8124 non-null object\n",
      "bruises                     8124 non-null int64\n",
      "odor                        8124 non-null object\n",
      "gill_attachment             8124 non-null object\n",
      "gill_spacing                8124 non-null object\n",
      "gill_size                   8124 non-null object\n",
      "gill_color                  8124 non-null object\n",
      "stalk_shape                 8124 non-null object\n",
      "stalk_root                  8124 non-null object\n",
      "stalk_surface_above_ring    8124 non-null object\n",
      "stalk_surface_below_ring    8124 non-null object\n",
      "stalk_color_above_ring      8124 non-null object\n",
      "stalk_color_below_ring      8124 non-null object\n",
      "veil_type                   8124 non-null object\n",
      "veil_color                  8124 non-null object\n",
      "ring_number                 8124 non-null int64\n",
      "ring_type                   8124 non-null object\n",
      "spore_print_color           8124 non-null object\n",
      "population                  8124 non-null object\n",
      "habitat                     8124 non-null object\n",
      "dtypes: int64(2), object(21)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 8124 rows in the data set, and that no rows have any missing columns. So, from that point, the data is good. Now we should check for duplicate rows in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = 0\n",
    "for val in df.duplicated():\n",
    "    if val == 'True':\n",
    "        duplicate_count += 1\n",
    "\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are no duplicate rows in this dataset either. \n",
    "\n",
    "The lack of duplicate rows and the fact that none of the columns are missing any values makes sense, as the description for the dataset says itself that the dataset has been lightly cleaned before publication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of poisonous mushrooms in set: 3916\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of poisonous mushrooms in set: {}\".format(len(df[df[\"class\"] == \"poisonous\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Criteria\n",
    "\n",
    "As we discussed in our opening analysis, the focus of this dataset would be to assist someone in determining if a mushroom is safe to eat or not based on its features. So, the important measure here would be the recall score of the classification. We don't necessarily care if the algorithm successfully classifies non-poisonous mushrooms. If a mushroom is not poisonous, and we say it is, all that means is that we are being more \"cautious\" with our recommendation of eating said mushroom. All we care about is that if we tell someone a mushroom is safe to eat, we are correct. We don't know what an acceptable margin of error would be here (we think it would be unreasonable to expect absolutely 100% recall on any possible dataset), but from our sample of >8,000 mushrooms with almost 4,000 poisonous mushrooms, we think that we should be able correctly classify every poisonous mushroom for our model to be even considered to be used in a real setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFXCAYAAAAoDt3iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a7e7de9f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grouping by odor and seeing the percentage of poisonous mushrooms\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "# df_grouped_odor = df.groupby(by=[\"odor\", \"class\"])\n",
    "\n",
    "sns.countplot(y=df.odor, hue=df[\"class\"], palette=\"muted\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this graph that a human could probably look at this graph and correctly classify a mushroom based on its odor. All mushrooms with a distinct odor fall into a single class. A decent machine learning model should be able to easily classify mushrooms with our dataset with the odor included. If it does too well, we may consider dropping the odor category from the dataset and seeing how the network does without such a clear indicator. This would be useful because someone may not be able to accurately identify the scent of a mushroom, either because it is too weak of a scent or the scent is not able to be identified. So, as we go, we may decide to leave out the odor from training and testing. \n",
    "\n",
    "When we have finished the general architecture for our networks, we may also consider removing some of the other classes from the set, to better mimic a deployment of an app to help you determine if a mushroom is poisonous. The app may be able to perform just as well with a subset of the classes, which would make filling out a form on the app easier and more likely to be used. Additionally, some of the values for these classes, like \"pendant\" or \"sheathing\" for the mushroom's ring type, might be too unfamiliar to a novice shroomer and thus not useful. If our model can still work with less features to train on, that would make it more accessible for general use. \n",
    "\n",
    "It is important to note that we have a limited dataset to work with. The dataset consists solely of samples from two genuses of mushrooms in North America. What we are doing here is testing if our model can work with data it's given and perform well by testing it on data collected from that same population. Thus, our model should not be deployed on a worldwide basis, given that we do not have data from worldwide species of mushrooms. If it is to be used in a worldwide setting, it should be given a worldwide data source, and would need to be retrained on that dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "For our validation technique, we will be using stratified 10-fold cross validation. First, it's important to look at the class distribution of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFXCAYAAABOYlxEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL6GbV2H07DYXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a7e716dba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=\"class\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset does not have a large class imbalance, so that removes one of the benefits for using a stratified method. However, another benefit of a stratified method is to ensure a level of uniformity in the training. We would like to train a model on as many differing datasets as possible, so we can do a reasonable job of estimating generalization performance. We do not have knowledge about the average distributions of poisonous mushrooms throughout the world. We do know that these mushrooms specifically came from species of North American mushrooms. We don't know how the data were collected, and so we will use the stratified method to preserve the class distributions, because we will assume that the samples are representative of the sample size. \n",
    "\n",
    "Now, why are we using 10-fold cross validation? We are using 10 folds because we feel that will give us a good trade-off between amount of training data and distributions of our scoring measure to evaluate the effectiveness of the models. One possible gotcha will be if the 10 folds can each capture a large enough portion of each of the categorical feature values to train up that set of weights. For example, as we saw earlier in the odor graph, there appear to be less than 100 instances that have a musty smell. It might be the case that some folds do not grab enough of that subset to be able to train its weight effectively. If our scoring measure is not as good as we would like, that might be one area to look at for training. That is, to stratify the dataset on the feature values as well as the classes. \n",
    "\n",
    "We will also note that using 10-fold cross validation is only our way of estimating generalization performance on the dataset. In our final deployment scenario, we would train a model on all of the data we have available to us and then ship that model off to our product, at which point we would measure its performance in the wild with entirely new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Now we will prepare the data for use in Tensorflow. First we will encode the categorical fields as integer values using sklearn's LabelEncoder, and we will scale the two numeric fields. Then, when we create the model, we will one hot encode the integer fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric headers: ['bruises', 'ring_number']\n",
      "Categorical headers: ['cap_shape', 'cap_surface', 'cap_color', 'odor', 'gill_attachment', 'gill_spacing', 'gill_size', 'gill_color', 'stalk_shape', 'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring', 'stalk_color_above_ring', 'stalk_color_below_ring', 'veil_type', 'veil_color', 'ring_type', 'spore_print_color', 'population', 'habitat']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap_shape</th>\n",
       "      <th>cap_surface</th>\n",
       "      <th>cap_color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill_attachment</th>\n",
       "      <th>gill_spacing</th>\n",
       "      <th>gill_size</th>\n",
       "      <th>gill_color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk_surface_above_ring_int</th>\n",
       "      <th>stalk_surface_below_ring_int</th>\n",
       "      <th>stalk_color_above_ring_int</th>\n",
       "      <th>stalk_color_below_ring_int</th>\n",
       "      <th>veil_type_int</th>\n",
       "      <th>veil_color_int</th>\n",
       "      <th>ring_type_int</th>\n",
       "      <th>spore_print_color_int</th>\n",
       "      <th>population_int</th>\n",
       "      <th>habitat_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>convex</td>\n",
       "      <td>smooth</td>\n",
       "      <td>brown</td>\n",
       "      <td>1.185917</td>\n",
       "      <td>pungent</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>narrow</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>convex</td>\n",
       "      <td>smooth</td>\n",
       "      <td>yellow</td>\n",
       "      <td>1.185917</td>\n",
       "      <td>almond</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>broad</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bell</td>\n",
       "      <td>smooth</td>\n",
       "      <td>white</td>\n",
       "      <td>1.185917</td>\n",
       "      <td>anise</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>broad</td>\n",
       "      <td>brown</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>convex</td>\n",
       "      <td>scaly</td>\n",
       "      <td>white</td>\n",
       "      <td>1.185917</td>\n",
       "      <td>pungent</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>narrow</td>\n",
       "      <td>brown</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>convex</td>\n",
       "      <td>smooth</td>\n",
       "      <td>gray</td>\n",
       "      <td>-0.843230</td>\n",
       "      <td>none</td>\n",
       "      <td>free</td>\n",
       "      <td>crowded</td>\n",
       "      <td>broad</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class cap_shape cap_surface cap_color   bruises     odor gill_attachment  \\\n",
       "0      1    convex      smooth     brown  1.185917  pungent            free   \n",
       "1      0    convex      smooth    yellow  1.185917   almond            free   \n",
       "2      0      bell      smooth     white  1.185917    anise            free   \n",
       "3      1    convex       scaly     white  1.185917  pungent            free   \n",
       "4      0    convex      smooth      gray -0.843230     none            free   \n",
       "\n",
       "  gill_spacing gill_size gill_color     ...     stalk_surface_above_ring_int  \\\n",
       "0        close    narrow      black     ...                                3   \n",
       "1        close     broad      black     ...                                3   \n",
       "2        close     broad      brown     ...                                3   \n",
       "3        close    narrow      brown     ...                                3   \n",
       "4      crowded     broad      black     ...                                3   \n",
       "\n",
       "  stalk_surface_below_ring_int stalk_color_above_ring_int  \\\n",
       "0                            3                          7   \n",
       "1                            3                          7   \n",
       "2                            3                          7   \n",
       "3                            3                          7   \n",
       "4                            3                          7   \n",
       "\n",
       "  stalk_color_below_ring_int veil_type_int veil_color_int ring_type_int  \\\n",
       "0                          7             0              2             4   \n",
       "1                          7             0              2             4   \n",
       "2                          7             0              2             4   \n",
       "3                          7             0              2             4   \n",
       "4                          7             0              2             0   \n",
       "\n",
       "  spore_print_color_int  population_int habitat_int  \n",
       "0                     0               3           4  \n",
       "1                     1               2           0  \n",
       "2                     1               2           2  \n",
       "3                     0               3           4  \n",
       "4                     1               0           0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "def create_new_data_frame(df, included_features, numeric_headers):\n",
    "#     numeric_headers = [\"bruises\", \"ring_number\"]\n",
    "    categorical_headers = [col for col in included_features if col not in numeric_headers]\n",
    "\n",
    "    df_processed = deepcopy(df)\n",
    "    all_features = included_features+[\"class\"]\n",
    "    drop_cols = [col for col in df.columns if col not in included_features+[\"class\"]]\n",
    "    df_processed = df_processed.drop(drop_cols, axis=1)\n",
    "    \n",
    "    print(\"Numeric headers: {}\".format(numeric_headers))\n",
    "    print(\"Categorical headers: {}\".format(categorical_headers))\n",
    "\n",
    "    encoders = dict() \n",
    "\n",
    "    for col in categorical_headers+['class']:\n",
    "#         df_processed[col] = df_processed[col].str.strip()\n",
    "\n",
    "        if col==\"class\":\n",
    "            tmp = LabelEncoder()\n",
    "            df_processed[col] = tmp.fit_transform(df_processed[col])\n",
    "        else:\n",
    "            encoders[col] = LabelEncoder()\n",
    "            df_processed[col+'_int'] = encoders[col].fit_transform(df_processed[col])\n",
    "\n",
    "\n",
    "    for col in numeric_headers:\n",
    "        df_processed[col] = df_processed[col].astype(np.float)\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        df_processed[col] = ss.fit_transform(df_processed[col].values.reshape(-1, 1))\n",
    "\n",
    "    categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "    feature_columns = categorical_headers_ints+numeric_headers\n",
    "    \n",
    "    return deepcopy(df_processed), categorical_headers_ints, numeric_headers, feature_columns\n",
    "    \n",
    "    \n",
    "numeric_headers = [\"bruises\", \"ring_number\"]\n",
    "included_features = list(df.columns)\n",
    "included_features.remove(\"class\")\n",
    "df_processed, categorical_headers_ints, numeric_headers, feature_columns = create_new_data_frame(df, included_features, numeric_headers)\n",
    "\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we create a single layer of the sparse and deep layer combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.layers import Embedding, Flatten, Merge, concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A combined wide and deep network\n",
    "\n",
    "Most of this code was taken from our boi Eric Larson's notebook \"10. Keras Wide and Deep\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "default_cross_columns = [['cap_shape','cap_surface', 'cap_color'],\n",
    "         ['gill_attachment', 'gill_spacing', 'gill_size', 'gill_color'],\n",
    "            ['stalk_shape', 'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring'],\n",
    "            ['veil_type', 'veil_color']]\n",
    "\n",
    "def create_model(X_train, X_test, X_train_num, categorical_headers_ints, cross_columns=default_cross_columns):\n",
    "#     we need to create separate sequential models for each embedding\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "        \n",
    "        # create crossed labels\n",
    "        X_crossed_train = X_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = X_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32',name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( X_train[col].values )\n",
    "        X_ints_test.append( X_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "        \n",
    "    # Make sure that X_train_num is ignored when empty\n",
    "    if X_train_num != []:\n",
    "        # also get a dense branch of the numeric features\n",
    "        all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    \n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='sigmoid')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='sigmoid')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "    return model, X_ints_train, X_ints_test, X_train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:63: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.1099 - acc: 0.9049     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0299 - acc: 0.9865     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0155 - acc: 0.9932     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0099 - acc: 0.9966     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0072 - acc: 0.9985     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0057 - acc: 0.9990     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0047 - acc: 0.9990     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0040 - acc: 0.9993     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0034 - acc: 0.9995     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0030 - acc: 0.9997     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0027 - acc: 0.9997     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0024 - acc: 0.9997     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0022 - acc: 0.9997     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0020 - acc: 0.9999     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0018 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 0.9999     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0016 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 1\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.1059 - acc: 0.9040     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0283 - acc: 0.9845     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0139 - acc: 0.9941     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0088 - acc: 0.9966     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0065 - acc: 0.9975     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0052 - acc: 0.9982     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0043 - acc: 0.9982     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0036 - acc: 0.9988     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0031 - acc: 0.9992     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0027 - acc: 0.9996     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0024 - acc: 0.9996     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0022 - acc: 0.9997     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0020 - acc: 0.9997     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0018 - acc: 0.9997     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0016 - acc: 0.9999     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 0.9999     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 2\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - ETA: 0s - loss: 0.1058 - acc: 0.908 - 2s - loss: 0.1014 - acc: 0.9136     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0277 - acc: 0.9848     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0141 - acc: 0.9926     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0090 - acc: 0.9956     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0064 - acc: 0.9974     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0050 - acc: 0.9979     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0041 - acc: 0.9986     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0034 - acc: 0.9992     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0029 - acc: 0.9995     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0026 - acc: 0.9996     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0023 - acc: 0.9996     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0020 - acc: 0.9997     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0018 - acc: 0.9997     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 0.9997     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 0.9997     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 0.9999     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 0.9999     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 0.9999     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 0.9999      \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 3\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.1111 - acc: 0.8981     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0306 - acc: 0.9854     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0152 - acc: 0.9933     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0096 - acc: 0.9971     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0070 - acc: 0.9975     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0055 - acc: 0.9985     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0045 - acc: 0.9993     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0038 - acc: 0.9993     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0033 - acc: 0.9995     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0029 - acc: 0.9995     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0025 - acc: 0.9996     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0023 - acc: 0.9997     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0021 - acc: 0.9999     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0019 - acc: 0.9999     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 0.9999     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0016 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 4\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 2s - loss: 0.1076 - acc: 0.9086     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0298 - acc: 0.9850     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0153 - acc: 0.9921     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0098 - acc: 0.9963     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0069 - acc: 0.9981     \n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7311/7311 [==============================] - 0s - loss: 0.0054 - acc: 0.9984     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0044 - acc: 0.9985     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0038 - acc: 0.9990     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0033 - acc: 0.9995     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0029 - acc: 0.9997     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0025 - acc: 0.9999     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0023 - acc: 0.9997     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0021 - acc: 0.9999     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0019 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0016 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 5\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 2s - loss: 0.1035 - acc: 0.9077     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0311 - acc: 0.9836     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0148 - acc: 0.9948     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0091 - acc: 0.9970     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0066 - acc: 0.9977     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0051 - acc: 0.9981     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0042 - acc: 0.9986     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0035 - acc: 0.9989     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0030 - acc: 0.9996     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0027 - acc: 0.9996     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0024 - acc: 0.9996     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0021 - acc: 0.9997     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0019 - acc: 0.9996     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 0.9999     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0016 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 6\n",
      "Epoch 1/20\n",
      "7312/7312 [==============================] - 1s - loss: 0.1063 - acc: 0.9033     \n",
      "Epoch 2/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0298 - acc: 0.9845     \n",
      "Epoch 3/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0145 - acc: 0.9940     \n",
      "Epoch 4/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0091 - acc: 0.9971     \n",
      "Epoch 5/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0067 - acc: 0.9977     \n",
      "Epoch 6/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0053 - acc: 0.9984     \n",
      "Epoch 7/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0043 - acc: 0.9988     \n",
      "Epoch 8/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0037 - acc: 0.9993     \n",
      "Epoch 9/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0032 - acc: 0.9996     \n",
      "Epoch 10/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0028 - acc: 0.9996     \n",
      "Epoch 11/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0025 - acc: 0.9997     \n",
      "Epoch 12/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0022 - acc: 0.9999     \n",
      "Epoch 13/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0020 - acc: 0.9999     \n",
      "Epoch 14/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0018 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0017 - acc: 0.9999     \n",
      "Epoch 16/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 7\n",
      "Epoch 1/20\n",
      "7312/7312 [==============================] - 1s - loss: 0.0999 - acc: 0.9171     \n",
      "Epoch 2/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0285 - acc: 0.9866     \n",
      "Epoch 3/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0142 - acc: 0.9943     \n",
      "Epoch 4/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0090 - acc: 0.9969     \n",
      "Epoch 5/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0065 - acc: 0.9982     \n",
      "Epoch 6/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0051 - acc: 0.9988     \n",
      "Epoch 7/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0041 - acc: 0.9992     \n",
      "Epoch 8/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0035 - acc: 0.9995     \n",
      "Epoch 9/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0030 - acc: 0.9996     \n",
      "Epoch 10/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0026 - acc: 0.9996     \n",
      "Epoch 11/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0023 - acc: 0.9997     \n",
      "Epoch 12/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0021 - acc: 0.9999     \n",
      "Epoch 13/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0019 - acc: 0.9999     \n",
      "Epoch 14/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0017 - acc: 0.9999     \n",
      "Epoch 15/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0016 - acc: 0.9999     \n",
      "Epoch 16/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "[[420   1]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 8\n",
      "Epoch 1/20\n",
      "7313/7313 [==============================] - 1s - loss: 0.0940 - acc: 0.9033     \n",
      "Epoch 2/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0238 - acc: 0.9889     \n",
      "Epoch 3/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0112 - acc: 0.9962     \n",
      "Epoch 4/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0073 - acc: 0.9969     \n",
      "Epoch 5/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0054 - acc: 0.9977     \n",
      "Epoch 6/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0043 - acc: 0.9981     \n",
      "Epoch 7/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0036 - acc: 0.9989     \n",
      "Epoch 8/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0030 - acc: 0.9993     \n",
      "Epoch 9/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0026 - acc: 0.9996     \n",
      "Epoch 10/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0023 - acc: 0.9996     \n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7313/7313 [==============================] - 0s - loss: 0.0020 - acc: 0.9997     \n",
      "Epoch 12/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0018 - acc: 0.9999     \n",
      "Epoch 13/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0016 - acc: 0.9999     \n",
      "Epoch 14/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0015 - acc: 0.9999     \n",
      "Epoch 15/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0014 - acc: 0.9999     \n",
      "Epoch 16/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0010 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7313/7313 [==============================] - 0s - loss: 9.4888e-04 - acc: 1.0000     \n",
      "[[420   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 9\n",
      "Epoch 1/20\n",
      "7313/7313 [==============================] - 1s - loss: 0.1068 - acc: 0.8994     \n",
      "Epoch 2/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0287 - acc: 0.9889     \n",
      "Epoch 3/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0145 - acc: 0.9959     \n",
      "Epoch 4/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0095 - acc: 0.9969     \n",
      "Epoch 5/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0069 - acc: 0.9979     \n",
      "Epoch 6/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0054 - acc: 0.9984     \n",
      "Epoch 7/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0043 - acc: 0.9990     \n",
      "Epoch 8/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0036 - acc: 0.9996     \n",
      "Epoch 9/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0031 - acc: 0.9996     \n",
      "Epoch 10/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0027 - acc: 0.9997     \n",
      "Epoch 11/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0024 - acc: 0.9999     \n",
      "Epoch 12/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0021 - acc: 0.9999     \n",
      "Epoch 13/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0019 - acc: 0.9999     \n",
      "Epoch 14/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0016 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "[[420   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_folds = 10\n",
    "\n",
    "X = df_processed[feature_columns].values\n",
    "y = df_processed[\"class\"].values\n",
    "\n",
    "scores = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "for i, (train, test) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    X_train = df_processed.iloc[train]\n",
    "    X_test = df_processed.iloc[test]\n",
    "    y_train = df_processed[\"class\"].iloc[train].values.astype(np.int)\n",
    "    y_test = df_processed[\"class\"].iloc[test].values.astype(np.int)\n",
    "    \n",
    "    ohe = OneHotEncoder()\n",
    "    X_train_ohe = ohe.fit_transform(X_train[categorical_headers_ints].values)\n",
    "    X_test_ohe = ohe.transform(X_test[categorical_headers_ints].values)\n",
    "\n",
    "    X_train_num = X_train[numeric_headers].values  # already scaled\n",
    "    X_test_num = X_test[numeric_headers].values\n",
    "    \n",
    "#     model = create_model(X_train_ohe, X_train_num)\n",
    "    model, X_ints_train, X_ints_test, X_train_num = create_model(X_train, X_test, X_train_num, categorical_headers_ints, default_cross_columns)\n",
    "    \n",
    "    model.compile(optimizer='adagrad',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "#     X_crossed_train, X_crossed_test, model = create_model_wide(X_train, X_test)\n",
    "#     model.fit([X_train_ohe,X_train_num],y_train, epochs=10, batch_size=50, verbose=0)\n",
    "    model.fit(X_ints_train + [X_train_num], y_train, epochs=20, batch_size=50, verbose=1)\n",
    "\n",
    "#     yhat = np.round(model.predict([X_test_ohe,X_test_num]))\n",
    "    yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "    print(mt.confusion_matrix(y_test,yhat))\n",
    "    score = mt.recall_score(y_test, yhat)\n",
    "    scores.append(score)\n",
    "    print(\"Recall: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#somewhat deep\n",
    "def create_somewhat_deep_model(X_train, X_test, X_train_num, categorical_headers_ints, cross_columns=default_cross_columns):\n",
    "#     we need to create separate sequential models for each embedding\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "        \n",
    "        # create crossed labels\n",
    "        X_crossed_train = X_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = X_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32',name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( X_train[col].values )\n",
    "        X_ints_test.append( X_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "        \n",
    "    # Make sure that X_train_num is ignored when empty\n",
    "    if X_train_num != []:\n",
    "        # also get a dense branch of the numeric features\n",
    "        all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    \n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='sigmoid')(deep_branch)\n",
    "    deep_branch = Dense(units=20,activation='sigmoid')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='sigmoid')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "    return model, X_ints_train, X_ints_test, X_train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:56: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.1204 - acc: 0.9073     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0331 - acc: 0.9829     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0138 - acc: 0.9956     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0086 - acc: 0.9978     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0063 - acc: 0.9982     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0050 - acc: 0.9985     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0042 - acc: 0.9988     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0036 - acc: 0.9988     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0032 - acc: 0.9988     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0029 - acc: 0.9989     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0026 - acc: 0.9990     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0024 - acc: 0.9990     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0022 - acc: 0.9990     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0021 - acc: 0.9990     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0020 - acc: 0.9990     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0018 - acc: 0.9990     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 0.9990     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 0.9993     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 0.9996     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 0.9999     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 1\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.1117 - acc: 0.9125     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0299 - acc: 0.9836     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0135 - acc: 0.9945     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0084 - acc: 0.9974     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0061 - acc: 0.9977     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0048 - acc: 0.9982     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0039 - acc: 0.9984     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0033 - acc: 0.9986     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0029 - acc: 0.9989     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0024 - acc: 0.9995     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0021 - acc: 0.9996     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0019 - acc: 0.9997     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 0.9999     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0010 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 9.5684e-04 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 2\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.1071 - acc: 0.9200     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0249 - acc: 0.9866     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0117 - acc: 0.9959     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0073 - acc: 0.9975     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0054 - acc: 0.9982     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0042 - acc: 0.9988     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0034 - acc: 0.9995     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0028 - acc: 0.9996     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0024 - acc: 0.9997     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0021 - acc: 0.9996     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0019 - acc: 0.9999     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0010 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 9.3960e-04 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 8.8394e-04 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 3\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.1051 - acc: 0.9404     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0291 - acc: 0.9908     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0135 - acc: 0.9956     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0085 - acc: 0.9978     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0062 - acc: 0.9981     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0049 - acc: 0.9989     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0039 - acc: 0.9996     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0033 - acc: 0.9997     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0028 - acc: 0.9999     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0024 - acc: 0.9999     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0021 - acc: 1.0000     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0019 - acc: 1.0000     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0016 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0010 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 4\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.1135 - acc: 0.9170     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0277 - acc: 0.9873     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0146 - acc: 0.9912     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0088 - acc: 0.9978     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0062 - acc: 0.9989     \n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7311/7311 [==============================] - 0s - loss: 0.0047 - acc: 0.9997     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0038 - acc: 0.9999     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0032 - acc: 1.0000     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0027 - acc: 0.9999     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0024 - acc: 1.0000     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0021 - acc: 0.9999     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.0019 - acc: 1.0000     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 1s - loss: 9.9658e-04 - acc: 1.0000     \n",
      "[[420   1]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 5\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.1108 - acc: 0.9063     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0269 - acc: 0.9873     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0120 - acc: 0.9949     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0077 - acc: 0.9974     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0057 - acc: 0.9982     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0046 - acc: 0.9985     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0038 - acc: 0.9985     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0033 - acc: 0.9986     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0029 - acc: 0.9986     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0025 - acc: 0.9988     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0022 - acc: 0.9992     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0019 - acc: 0.9999     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0015 - acc: 0.9999     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0010 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 9.4613e-04 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 6\n",
      "Epoch 1/20\n",
      "7312/7312 [==============================] - 2s - loss: 0.1102 - acc: 0.9063     \n",
      "Epoch 2/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0262 - acc: 0.9871     \n",
      "Epoch 3/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0126 - acc: 0.9948     \n",
      "Epoch 4/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0075 - acc: 0.9981     \n",
      "Epoch 5/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0054 - acc: 0.9986     \n",
      "Epoch 6/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0042 - acc: 0.9989     \n",
      "Epoch 7/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0035 - acc: 0.9996     \n",
      "Epoch 8/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0029 - acc: 0.9996     \n",
      "Epoch 9/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0025 - acc: 0.9999     \n",
      "Epoch 10/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0022 - acc: 0.9997     \n",
      "Epoch 11/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0019 - acc: 0.9997     \n",
      "Epoch 12/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 13/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0016 - acc: 0.9999     \n",
      "Epoch 14/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7312/7312 [==============================] - 0s - loss: 9.8854e-04 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7312/7312 [==============================] - 0s - loss: 9.3141e-04 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 7\n",
      "Epoch 1/20\n",
      "7312/7312 [==============================] - 1s - loss: 0.1099 - acc: 0.9259     \n",
      "Epoch 2/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0247 - acc: 0.9882     \n",
      "Epoch 3/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0131 - acc: 0.9904     \n",
      "Epoch 4/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0083 - acc: 0.9960     \n",
      "Epoch 5/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0055 - acc: 0.9985     \n",
      "Epoch 6/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0042 - acc: 0.9993     \n",
      "Epoch 7/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0034 - acc: 0.9995     \n",
      "Epoch 8/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0028 - acc: 0.9996     \n",
      "Epoch 9/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0024 - acc: 0.9997     \n",
      "Epoch 10/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0021 - acc: 0.9999     \n",
      "Epoch 11/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0018 - acc: 0.9999     \n",
      "Epoch 12/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0016 - acc: 0.9999     \n",
      "Epoch 13/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 14/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0010 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7312/7312 [==============================] - 0s - loss: 9.7350e-04 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7312/7312 [==============================] - 0s - loss: 9.1186e-04 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7312/7312 [==============================] - 0s - loss: 8.5619e-04 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 8\n",
      "Epoch 1/20\n",
      "7313/7313 [==============================] - 2s - loss: 0.1126 - acc: 0.9155     \n",
      "Epoch 2/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0280 - acc: 0.9881     \n",
      "Epoch 3/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0132 - acc: 0.9955     \n",
      "Epoch 4/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0082 - acc: 0.9977     \n",
      "Epoch 5/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0061 - acc: 0.9984     \n",
      "Epoch 6/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0048 - acc: 0.9986     \n",
      "Epoch 7/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0039 - acc: 0.9988     \n",
      "Epoch 8/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0033 - acc: 0.9989     \n",
      "Epoch 9/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0028 - acc: 0.9992     \n",
      "Epoch 10/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0025 - acc: 0.9996     \n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7313/7313 [==============================] - 0s - loss: 0.0021 - acc: 1.0000     \n",
      "Epoch 12/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0019 - acc: 1.0000     \n",
      "Epoch 13/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0017 - acc: 1.0000     \n",
      "Epoch 14/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0015 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7313/7313 [==============================] - 0s - loss: 9.9025e-04 - acc: 1.0000     \n",
      "[[420   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 9\n",
      "Epoch 1/20\n",
      "7313/7313 [==============================] - 2s - loss: 0.1040 - acc: 0.9293     \n",
      "Epoch 2/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0246 - acc: 0.9907     \n",
      "Epoch 3/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0124 - acc: 0.9952     \n",
      "Epoch 4/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0076 - acc: 0.9977     \n",
      "Epoch 5/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0056 - acc: 0.9981     \n",
      "Epoch 6/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0045 - acc: 0.9986     \n",
      "Epoch 7/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0037 - acc: 0.9988     \n",
      "Epoch 8/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0032 - acc: 0.9988     \n",
      "Epoch 9/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0027 - acc: 0.9993     \n",
      "Epoch 10/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0023 - acc: 0.9996     \n",
      "Epoch 11/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0020 - acc: 1.0000     \n",
      "Epoch 12/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0018 - acc: 1.0000     \n",
      "Epoch 13/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0016 - acc: 1.0000     \n",
      "Epoch 14/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0014 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0013 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0012 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0011 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7313/7313 [==============================] - 0s - loss: 9.8792e-04 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7313/7313 [==============================] - 0s - loss: 9.2905e-04 - acc: 1.0000     \n",
      "[[420   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_folds = 10\n",
    "\n",
    "X = df_processed[feature_columns].values\n",
    "y = df_processed[\"class\"].values\n",
    "\n",
    "some_scores = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "for i, (train, test) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    X_train = df_processed.iloc[train]\n",
    "    X_test = df_processed.iloc[test]\n",
    "    y_train = df_processed[\"class\"].iloc[train].values.astype(np.int)\n",
    "    y_test = df_processed[\"class\"].iloc[test].values.astype(np.int)\n",
    "    \n",
    "    ohe = OneHotEncoder()\n",
    "    X_train_ohe = ohe.fit_transform(X_train[categorical_headers_ints].values)\n",
    "    X_test_ohe = ohe.transform(X_test[categorical_headers_ints].values)\n",
    "\n",
    "    X_train_num = X_train[numeric_headers].values  # already scaled\n",
    "    X_test_num = X_test[numeric_headers].values\n",
    "    \n",
    "#     model = create_model(X_train_ohe, X_train_num)\n",
    "    model, X_ints_train, X_ints_test, X_train_num = create_somewhat_deep_model(X_train, X_test, X_train_num, categorical_headers_ints, default_cross_columns)\n",
    "    \n",
    "    model.compile(optimizer='adagrad',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "#     X_crossed_train, X_crossed_test, model = create_model_wide(X_train, X_test)\n",
    "#     model.fit([X_train_ohe,X_train_num],y_train, epochs=10, batch_size=50, verbose=0)\n",
    "    model.fit(X_ints_train + [X_train_num], y_train, epochs=20, batch_size=50, verbose=1)\n",
    "\n",
    "#     yhat = np.round(model.predict([X_test_ohe,X_test_num]))\n",
    "    yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "    print(mt.confusion_matrix(y_test,yhat))\n",
    "    score = mt.recall_score(y_test, yhat)\n",
    "    some_scores.append(score)\n",
    "    print(\"Recall: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_really_deep_model(X_train, X_test, X_train_num, categorical_headers_ints, cross_columns=default_cross_columns):\n",
    "#     we need to create separate sequential models for each embedding\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "        \n",
    "        # create crossed labels\n",
    "        X_crossed_train = X_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = X_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32',name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( X_train[col].values )\n",
    "        X_ints_test.append( X_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "        \n",
    "    # Make sure that X_train_num is ignored when empty\n",
    "    if X_train_num != []:\n",
    "        # also get a dense branch of the numeric features\n",
    "        all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    \n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='sigmoid')(deep_branch)\n",
    "    deep_branch = Dense(units=20,activation='sigmoid')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='sigmoid')(deep_branch)\n",
    "    deep_branch = Dense(units=5,activation='sigmoid')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "    return model, X_ints_train, X_ints_test, X_train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:55: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 2s - loss: 0.1335 - acc: 0.9283     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0527 - acc: 0.9788     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0284 - acc: 0.9891     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0193 - acc: 0.9926     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0148 - acc: 0.9948     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0122 - acc: 0.9955     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0101 - acc: 0.9958     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0083 - acc: 0.9963     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0073 - acc: 0.9971     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0064 - acc: 0.9970     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0058 - acc: 0.9978     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0053 - acc: 0.9981     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0048 - acc: 0.9985     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0045 - acc: 0.9985     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0042 - acc: 0.9985     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0039 - acc: 0.9989     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0037 - acc: 0.9989     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0035 - acc: 0.9990     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0033 - acc: 0.9990     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0032 - acc: 0.9990     \n",
      "[[421   0]\n",
      " [  1 391]]\n",
      "Recall:  0.997448979592\n",
      "Fold 1\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 2s - loss: 0.1188 - acc: 0.9331     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0364 - acc: 0.9880     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0206 - acc: 0.9917     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0150 - acc: 0.9941     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0120 - acc: 0.9952     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0100 - acc: 0.9956     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0087 - acc: 0.9964     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0075 - acc: 0.9974     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0062 - acc: 0.9973     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0054 - acc: 0.9979     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0048 - acc: 0.9985     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0044 - acc: 0.9992     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0040 - acc: 0.9996     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0037 - acc: 0.9999     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0034 - acc: 0.9999     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0032 - acc: 0.9999     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0030 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0028 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0026 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0025 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 2\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 2s - loss: 0.1223 - acc: 0.9367     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0416 - acc: 0.9852     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0215 - acc: 0.9910     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0152 - acc: 0.9937     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0120 - acc: 0.9947     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0100 - acc: 0.9951     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0087 - acc: 0.9967     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0077 - acc: 0.9970     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0070 - acc: 0.9973     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0064 - acc: 0.9974     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0059 - acc: 0.9981     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0055 - acc: 0.9978     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0052 - acc: 0.9978     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0049 - acc: 0.9975     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0046 - acc: 0.9977     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0044 - acc: 0.9974     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0042 - acc: 0.9978     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0040 - acc: 0.9982     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0039 - acc: 0.9981     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0036 - acc: 0.9982     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 3\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 2s - loss: 0.1425 - acc: 0.9136     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0516 - acc: 0.9852     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0265 - acc: 0.9936     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0181 - acc: 0.9948     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0140 - acc: 0.9947     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0115 - acc: 0.9953     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0099 - acc: 0.9963     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0087 - acc: 0.9964     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0078 - acc: 0.9973     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0072 - acc: 0.9975     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0066 - acc: 0.9978     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0061 - acc: 0.9985     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0057 - acc: 0.9985     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0054 - acc: 0.9986     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0051 - acc: 0.9985     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0049 - acc: 0.9984     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0046 - acc: 0.9982     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0044 - acc: 0.9979     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0042 - acc: 0.9978     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0040 - acc: 0.9979     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 4\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 2s - loss: 0.1341 - acc: 0.9134     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0497 - acc: 0.9841     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0274 - acc: 0.9896     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0195 - acc: 0.9930     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.0153 - acc: 0.9949     \n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7311/7311 [==============================] - 1s - loss: 0.0127 - acc: 0.9953     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0110 - acc: 0.9956     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0097 - acc: 0.9958     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0087 - acc: 0.9962     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0079 - acc: 0.9960     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0073 - acc: 0.9964     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0068 - acc: 0.9964     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0063 - acc: 0.9966     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0059 - acc: 0.9973     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0056 - acc: 0.9977     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0053 - acc: 0.9977     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0050 - acc: 0.9978     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0047 - acc: 0.9981     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0044 - acc: 0.9986     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0042 - acc: 0.9986     \n",
      "[[420   1]\n",
      " [  2 390]]\n",
      "Recall:  0.994897959184\n",
      "Fold 5\n",
      "Epoch 1/20\n",
      "7311/7311 [==============================] - 2s - loss: 0.1286 - acc: 0.9185     \n",
      "Epoch 2/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0436 - acc: 0.9840     \n",
      "Epoch 3/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0242 - acc: 0.9893     \n",
      "Epoch 4/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0173 - acc: 0.9925     \n",
      "Epoch 5/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0137 - acc: 0.9941     \n",
      "Epoch 6/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0114 - acc: 0.9949     \n",
      "Epoch 7/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0098 - acc: 0.9960     \n",
      "Epoch 8/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0087 - acc: 0.9960     \n",
      "Epoch 9/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0078 - acc: 0.9964     \n",
      "Epoch 10/20\n",
      "7311/7311 [==============================] - 1s - loss: 0.0071 - acc: 0.9966     \n",
      "Epoch 11/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0066 - acc: 0.9971     \n",
      "Epoch 12/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0061 - acc: 0.9975     \n",
      "Epoch 13/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0057 - acc: 0.9982     \n",
      "Epoch 14/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0054 - acc: 0.9986     \n",
      "Epoch 15/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0050 - acc: 0.9986     \n",
      "Epoch 16/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0042 - acc: 0.9989     \n",
      "Epoch 17/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0038 - acc: 0.9989     \n",
      "Epoch 18/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0035 - acc: 0.9989     \n",
      "Epoch 19/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0033 - acc: 0.9993     \n",
      "Epoch 20/20\n",
      "7311/7311 [==============================] - 0s - loss: 0.0031 - acc: 0.9995     \n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 6\n",
      "Epoch 1/20\n",
      "7312/7312 [==============================] - 3s - loss: 0.1297 - acc: 0.9225     \n",
      "Epoch 2/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0464 - acc: 0.9850     \n",
      "Epoch 3/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0254 - acc: 0.9919     \n",
      "Epoch 4/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0179 - acc: 0.9951     \n",
      "Epoch 5/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0141 - acc: 0.9958     \n",
      "Epoch 6/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0118 - acc: 0.9963     \n",
      "Epoch 7/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0102 - acc: 0.9964     \n",
      "Epoch 8/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0090 - acc: 0.9967     \n",
      "Epoch 9/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0081 - acc: 0.9969     \n",
      "Epoch 10/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0074 - acc: 0.9969     \n",
      "Epoch 11/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0068 - acc: 0.9969     \n",
      "Epoch 12/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0064 - acc: 0.9967     - ETA: 0s - loss: 0.0072\n",
      "Epoch 13/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0060 - acc: 0.9971     \n",
      "Epoch 14/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0056 - acc: 0.9974     \n",
      "Epoch 15/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0052 - acc: 0.9977     \n",
      "Epoch 16/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0049 - acc: 0.9979     \n",
      "Epoch 17/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0046 - acc: 0.9979     \n",
      "Epoch 18/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0044 - acc: 0.9984     \n",
      "Epoch 19/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0042 - acc: 0.9984     \n",
      "Epoch 20/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0041 - acc: 0.9985     \n",
      "[[420   1]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 7\n",
      "Epoch 1/20\n",
      "7312/7312 [==============================] - 3s - loss: 0.1288 - acc: 0.9326     \n",
      "Epoch 2/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0424 - acc: 0.9871     \n",
      "Epoch 3/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0225 - acc: 0.9918     \n",
      "Epoch 4/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0159 - acc: 0.9933     \n",
      "Epoch 5/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0126 - acc: 0.9937     \n",
      "Epoch 6/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0105 - acc: 0.9954     \n",
      "Epoch 7/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0084 - acc: 0.9969     \n",
      "Epoch 8/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0070 - acc: 0.9978     \n",
      "Epoch 9/20\n",
      "7312/7312 [==============================] - 1s - loss: 0.0061 - acc: 0.9985     \n",
      "Epoch 10/20\n",
      "7312/7312 [==============================] - 1s - loss: 0.0054 - acc: 0.9993     \n",
      "Epoch 11/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0048 - acc: 0.9995     \n",
      "Epoch 12/20\n",
      "7312/7312 [==============================] - 1s - loss: 0.0043 - acc: 0.9996     \n",
      "Epoch 13/20\n",
      "7312/7312 [==============================] - 1s - loss: 0.0039 - acc: 0.9999     \n",
      "Epoch 14/20\n",
      "7312/7312 [==============================] - 1s - loss: 0.0036 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0034 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "7312/7312 [==============================] - 1s - loss: 0.0031 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0029 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0028 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0026 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "7312/7312 [==============================] - 0s - loss: 0.0025 - acc: 1.0000     \n",
      "[[421   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 8\n",
      "Epoch 1/20\n",
      "7313/7313 [==============================] - 4s - loss: 0.1183 - acc: 0.9474     \n",
      "Epoch 2/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0423 - acc: 0.9819     \n",
      "Epoch 3/20\n",
      "7313/7313 [==============================] - 1s - loss: 0.0211 - acc: 0.9929     \n",
      "Epoch 4/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0138 - acc: 0.9966     \n",
      "Epoch 5/20\n",
      "7313/7313 [==============================] - 1s - loss: 0.0104 - acc: 0.9969     \n",
      "Epoch 6/20\n",
      "7313/7313 [==============================] - 1s - loss: 0.0084 - acc: 0.9977     \n",
      "Epoch 7/20\n",
      "7313/7313 [==============================] - 2s - loss: 0.0071 - acc: 0.9982     \n",
      "Epoch 8/20\n",
      "7313/7313 [==============================] - 1s - loss: 0.0062 - acc: 0.9982     \n",
      "Epoch 9/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0054 - acc: 0.9986     \n",
      "Epoch 10/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0049 - acc: 0.9989     \n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7313/7313 [==============================] - 0s - loss: 0.0045 - acc: 0.9989     \n",
      "Epoch 12/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0041 - acc: 0.9989     \n",
      "Epoch 13/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0038 - acc: 0.9990     \n",
      "Epoch 14/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0035 - acc: 0.9990     \n",
      "Epoch 15/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0033 - acc: 0.9990     \n",
      "Epoch 16/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0031 - acc: 0.9990     \n",
      "Epoch 17/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0029 - acc: 0.9990     \n",
      "Epoch 18/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0028 - acc: 0.9990     \n",
      "Epoch 19/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0027 - acc: 0.9990     - ETA: 0s - loss: 0.0030 \n",
      "Epoch 20/20\n",
      "7313/7313 [==============================] - 1s - loss: 0.0025 - acc: 0.9990     \n",
      "[[420   0]\n",
      " [  1 390]]\n",
      "Recall:  0.997442455243\n",
      "Fold 9\n",
      "Epoch 1/20\n",
      "7313/7313 [==============================] - 2s - loss: 0.1274 - acc: 0.9484     \n",
      "Epoch 2/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0460 - acc: 0.9835     \n",
      "Epoch 3/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0248 - acc: 0.9908     \n",
      "Epoch 4/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0177 - acc: 0.9954     \n",
      "Epoch 5/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0141 - acc: 0.9956     \n",
      "Epoch 6/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0118 - acc: 0.9959     \n",
      "Epoch 7/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0102 - acc: 0.9960     \n",
      "Epoch 8/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0090 - acc: 0.9960     \n",
      "Epoch 9/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0081 - acc: 0.9962     \n",
      "Epoch 10/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0073 - acc: 0.9964     \n",
      "Epoch 11/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0067 - acc: 0.9970     \n",
      "Epoch 12/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0061 - acc: 0.9979     \n",
      "Epoch 13/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0057 - acc: 0.9982     \n",
      "Epoch 14/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0053 - acc: 0.9982     \n",
      "Epoch 15/20\n",
      "7313/7313 [==============================] - 1s - loss: 0.0050 - acc: 0.9982     \n",
      "Epoch 16/20\n",
      "7313/7313 [==============================] - 1s - loss: 0.0047 - acc: 0.9982     \n",
      "Epoch 17/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0045 - acc: 0.9982     \n",
      "Epoch 18/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0043 - acc: 0.9982     \n",
      "Epoch 19/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0040 - acc: 0.9982     \n",
      "Epoch 20/20\n",
      "7313/7313 [==============================] - 0s - loss: 0.0038 - acc: 0.9984     \n",
      "[[420   0]\n",
      " [  1 390]]\n",
      "Recall:  0.997442455243\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_folds = 10\n",
    "\n",
    "X = df_processed[feature_columns].values\n",
    "y = df_processed[\"class\"].values\n",
    "\n",
    "deep_scores = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "for i, (train, test) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    X_train = df_processed.iloc[train]\n",
    "    X_test = df_processed.iloc[test]\n",
    "    y_train = df_processed[\"class\"].iloc[train].values.astype(np.int)\n",
    "    y_test = df_processed[\"class\"].iloc[test].values.astype(np.int)\n",
    "    \n",
    "    ohe = OneHotEncoder()\n",
    "    X_train_ohe = ohe.fit_transform(X_train[categorical_headers_ints].values)\n",
    "    X_test_ohe = ohe.transform(X_test[categorical_headers_ints].values)\n",
    "\n",
    "    X_train_num = X_train[numeric_headers].values  # already scaled\n",
    "    X_test_num = X_test[numeric_headers].values\n",
    "    \n",
    "#     model = create_model(X_train_ohe, X_train_num)\n",
    "    model, X_ints_train, X_ints_test, X_train_num = create_really_deep_model(X_train, X_test, X_train_num, categorical_headers_ints, default_cross_columns)\n",
    "    \n",
    "    model.compile(optimizer='adagrad',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "#     X_crossed_train, X_crossed_test, model = create_model_wide(X_train, X_test)\n",
    "#     model.fit([X_train_ohe,X_train_num],y_train, epochs=10, batch_size=50, verbose=0)\n",
    "    model.fit(X_ints_train + [X_train_num], y_train, epochs=20, batch_size=50, verbose=1)\n",
    "\n",
    "#     yhat = np.round(model.predict([X_test_ohe,X_test_num]))\n",
    "    yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "    print(mt.confusion_matrix(y_test,yhat))\n",
    "    score = mt.recall_score(y_test, yhat)\n",
    "    deep_scores.append(score)\n",
    "    print(\"Recall: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Container object of 10 artists>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFJCAYAAACsBZWNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a78d12a6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds = list(range(0,num_folds))\n",
    "print(scores)\n",
    "\n",
    "plt.bar(folds, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Container object of 10 artists>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFJCAYAAACsBZWNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a7cd61e0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds = list(range(0,num_folds))\n",
    "print(some_scores)\n",
    "\n",
    "plt.bar(folds, some_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99744897959183676, 1.0, 1.0, 1.0, 0.99489795918367352, 1.0, 1.0, 1.0, 0.99744245524296671, 0.99744245524296671]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Container object of 10 artists>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFJCAYAAACsBZWNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niQgDQJL/BT/MaJPv1Tm2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a7e9d59f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds = list(range(0,num_folds))\n",
    "print(deep_scores)\n",
    "\n",
    "plt.bar(folds, deep_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the layers increment we can observe worse time performance and slghtly worse prediction. The best performance in terms of time and scores is the initial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compare with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 1\n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 2\n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 3\n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 4\n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 5\n",
      "[[421   0]\n",
      " [  0 392]]\n",
      "Recall:  1.0\n",
      "Fold 6\n",
      "[[421   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 7\n",
      "[[421   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 8\n",
      "[[420   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Fold 9\n",
      "[[420   0]\n",
      " [  0 391]]\n",
      "Recall:  1.0\n",
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_folds = 10\n",
    "\n",
    "X = df_processed[feature_columns].values\n",
    "y = df_processed[\"class\"].values\n",
    "\n",
    "mlp_scores = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "for i, (train, test) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    X_train = df_processed.iloc[train]\n",
    "    X_test = df_processed.iloc[test]\n",
    "    y_train = df_processed[\"class\"].iloc[train].values.astype(np.int)\n",
    "    y_test = df_processed[\"class\"].iloc[test].values.astype(np.int)\n",
    "    \n",
    "    ohe = OneHotEncoder()\n",
    "    X_train_ohe = ohe.fit_transform(X_train[categorical_headers_ints].values)\n",
    "    X_test_ohe = ohe.transform(X_test[categorical_headers_ints].values)\n",
    "\n",
    "    X_train_num = X_train[numeric_headers].values  # already scaled\n",
    "    X_test_num = X_test[numeric_headers].values\n",
    "    \n",
    "# #     model = create_model(X_train_ohe, X_train_num)\n",
    "#     model, X_ints_train, X_ints_test, X_train_num = create_really_deep_model(X_train, X_test, X_train_num, categorical_headers_ints, default_cross_columns)\n",
    "    \n",
    "#     model.compile(optimizer='adagrad',\n",
    "#                   loss='mean_squared_error',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "#     model.fit(X_ints_train + [X_train_num], y_train, epochs=20, batch_size=50, verbose=1)\n",
    "\n",
    "    mlp.fit(X_train_ohe, y_train)\n",
    "\n",
    "    yhat = mlp.predict(X_test_ohe)\n",
    "    print(mt.confusion_matrix(y_test,yhat))\n",
    "    score = mt.recall_score(y_test, yhat)\n",
    "    mlp_scores.append(score)\n",
    "    print(\"Recall: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP has 1.0 as its recall score which matches that of Our best performing DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work : Weight Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap_shape_int\n",
      "cap_surface_int\n",
      "cap_color_int\n",
      "odor_int\n",
      "gill_attachment_int\n",
      "gill_spacing_int\n",
      "gill_size_int\n",
      "gill_color_int\n",
      "stalk_shape_int\n",
      "stalk_root_int\n",
      "stalk_surface_above_ring_int\n",
      "stalk_surface_below_ring_int\n",
      "stalk_color_above_ring_int\n",
      "stalk_color_below_ring_int\n",
      "veil_type_int\n",
      "veil_color_int\n",
      "ring_type_int\n",
      "spore_print_color_int\n",
      "population_int\n",
      "habitat_int\n"
     ]
    }
   ],
   "source": [
    "#Get weights for categorical variables\n",
    "weights_int = []\n",
    "for ind in model.layers:\n",
    "    if '_int' in ind.name:\n",
    "        print(ind.name)\n",
    "        weights_int.append(ind.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial model was good. However, our deployment is in an app where people enter the values of these features. Some of these features may be subjective and could potentially introduce a human error into the mix. We should take a closer at these features and try out our model on a subset of features, and only include features we think are objective or less likely to be messed up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            Feature             | Sample Values                       |               Explanation                | Include | Sample Image |\n",
    "| :----------------------------: | :---------------------------------- | :--------------------------------------: | :-----: | :----------: |\n",
    "|           Cap Shape            | Bell<br/>Conical<br/>Convex         |          Fairly objective trait          |  True  | ![cap_shape](img/cap_shape.jpg) |\n",
    "|           Cap Color            | Red<br/>Brown<br/>Cinammon          |     Difficult to tell apart colors like brown vs cinammon by eyes     |  False  |![brown](img/brown-fibrous.jpg)|\n",
    "|          Cap Surface           | Fibrous<br/>Smooth<br/>Scaly        |         Easily distinguishiable          |  True   | ![cap_surface](img/cap_surface.jpg)  |\n",
    "|            Bruises             | Exist<br/>Not Exist                 |  Non Numeric Feature, Easily Noticeable by spots when pressed  |  True   | ![bruise](img/bruise.jpg) |\n",
    "|              Odor              | Anise<br/>Cresote<br/>Pungent       | Too specific for untrained noses, easily confused |  False  |  N/A          |\n",
    "|        Gill Attachment         | Attached<br/>Descending<br/>Free    |     Will simplify to attached or Not     |  True   | ![gill attach](img/gil_attach.jpg) |\n",
    "|          Gill Spacing          | Close<br/>Crowded<br/>Distant       | Too detailed for untrained eyes, adjust to close/distant |  True   | ![title](img/gil_spacing.jpg) |\n",
    "|           Gill Size            | Broad<br/>Narrow                    | We can't define what is broad or narrow, and images on the net don't provide much assistance. If we had examples we could include it, but we don't so we will leave out gill size. |  False  | N/A    |\n",
    "|          Stalk Shape           | Enlarging<br/>Tapering              |      Easily noticeable difference.       |  True   | ![stalk shape](img/stalk_shape.jpg) |\n",
    "|           Stalk Root           | Bulbous<br/>Club<br/>Cup<br/>       | With pictures it is easily distinguishable |  True   | ![stalk root](img/stalk_root.jpg) |\n",
    "| Stalk Surface Above/Below Ring | Fibrous<br/>Scaly<br/>Smooth<br/>   | With assistance of image easily distinguishable. |  True   | ![stalk surface](img/stalk_surface.jpg)        |\n",
    "|  Stalk Color Above/Below Ring  |  Brown<br/>Buff<br/>Cinammon        |    Difficult to tell the color apart     |  False  | ![stalk color](img/stalk_color.jpg) |\n",
    "|           Veil Type            | Partial<br/>Universal               | According to https://en.wikipedia.org/wiki/Partial_veil, a mushroom can have a partial and/or universal veil so it's not a distinguishable factor. |  False  | ![veil type](img/veil_type.jpg) |\n",
    "|           Veil Color           | Brown<br/>Orange<br/>White          |      Easy to tell apart the colors       |  True   |![veil color](img/veil_color.jpg)  |\n",
    "|           Ring Type            | Cobwebby<br/>Evanescent<br/>Flaring |    With image easily distinguishable     |  True   | ![ring type](img/ring_type.jpg) |\n",
    "|          Ring Number           | 0<br/>1<br/>2                       | Most of the mushrooms have 1 ring, with its ratio of poisonous vs edible essentially being chance but this is easily distinguishable |  True  |  N/A  |\n",
    "|       Spore Print Color        | Buff<br/>Brown<br/>Chocolate        | Very hard to distinguish these colors and are up to interpretation. |  False  | ![spore color](img/spore_color.jpg) |\n",
    "|           Population           | Abandant<br/>Clustured<br/>Numerous | Difficult to verify the quantities, but with image assistance will be distinguishable |  False   | N/A so we cannot verify this  |\n",
    "|            Habitat             | Grass<br/>Path<br/>Waste            |     Easy to distinguish the terrain      |  True   | N/A |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric headers: ['ring_number']\n",
      "Categorical headers: ['cap_shape', 'cap_surface', 'bruises', 'gill_attachment', 'gill_spacing', 'stalk_shape', 'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring', 'veil_color', 'ring_type', 'habitat']\n",
      "['cap_shape_int', 'cap_surface_int', 'bruises_int', 'gill_attachment_int', 'gill_spacing_int', 'stalk_shape_int', 'stalk_root_int', 'stalk_surface_above_ring_int', 'stalk_surface_below_ring_int', 'veil_color_int', 'ring_type_int', 'habitat_int']\n",
      "['cap_shape_int', 'cap_surface_int', 'bruises_int', 'gill_attachment_int', 'gill_spacing_int', 'stalk_shape_int', 'stalk_root_int', 'stalk_surface_above_ring_int', 'stalk_surface_below_ring_int', 'veil_color_int', 'ring_type_int', 'habitat_int', 'ring_number']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap_shape</th>\n",
       "      <th>cap_surface</th>\n",
       "      <th>bruises</th>\n",
       "      <th>gill_attachment</th>\n",
       "      <th>gill_spacing</th>\n",
       "      <th>stalk_shape</th>\n",
       "      <th>stalk_root</th>\n",
       "      <th>stalk_surface_above_ring</th>\n",
       "      <th>stalk_surface_below_ring</th>\n",
       "      <th>...</th>\n",
       "      <th>bruises_int</th>\n",
       "      <th>gill_attachment_int</th>\n",
       "      <th>gill_spacing_int</th>\n",
       "      <th>stalk_shape_int</th>\n",
       "      <th>stalk_root_int</th>\n",
       "      <th>stalk_surface_above_ring_int</th>\n",
       "      <th>stalk_surface_below_ring_int</th>\n",
       "      <th>veil_color_int</th>\n",
       "      <th>ring_type_int</th>\n",
       "      <th>habitat_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>convex</td>\n",
       "      <td>smooth</td>\n",
       "      <td>1</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>enlarging</td>\n",
       "      <td>equal</td>\n",
       "      <td>smooth</td>\n",
       "      <td>smooth</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>convex</td>\n",
       "      <td>smooth</td>\n",
       "      <td>1</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>enlarging</td>\n",
       "      <td>club</td>\n",
       "      <td>smooth</td>\n",
       "      <td>smooth</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bell</td>\n",
       "      <td>smooth</td>\n",
       "      <td>1</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>enlarging</td>\n",
       "      <td>club</td>\n",
       "      <td>smooth</td>\n",
       "      <td>smooth</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>convex</td>\n",
       "      <td>scaly</td>\n",
       "      <td>1</td>\n",
       "      <td>free</td>\n",
       "      <td>close</td>\n",
       "      <td>enlarging</td>\n",
       "      <td>equal</td>\n",
       "      <td>smooth</td>\n",
       "      <td>smooth</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>convex</td>\n",
       "      <td>smooth</td>\n",
       "      <td>0</td>\n",
       "      <td>free</td>\n",
       "      <td>crowded</td>\n",
       "      <td>tapering</td>\n",
       "      <td>equal</td>\n",
       "      <td>smooth</td>\n",
       "      <td>smooth</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class cap_shape cap_surface  bruises gill_attachment gill_spacing  \\\n",
       "0      1    convex      smooth        1            free        close   \n",
       "1      0    convex      smooth        1            free        close   \n",
       "2      0      bell      smooth        1            free        close   \n",
       "3      1    convex       scaly        1            free        close   \n",
       "4      0    convex      smooth        0            free      crowded   \n",
       "\n",
       "  stalk_shape stalk_root stalk_surface_above_ring stalk_surface_below_ring  \\\n",
       "0   enlarging      equal                   smooth                   smooth   \n",
       "1   enlarging       club                   smooth                   smooth   \n",
       "2   enlarging       club                   smooth                   smooth   \n",
       "3   enlarging      equal                   smooth                   smooth   \n",
       "4    tapering      equal                   smooth                   smooth   \n",
       "\n",
       "      ...      bruises_int  gill_attachment_int gill_spacing_int  \\\n",
       "0     ...                1                    1                0   \n",
       "1     ...                1                    1                0   \n",
       "2     ...                1                    1                0   \n",
       "3     ...                1                    1                0   \n",
       "4     ...                0                    1                1   \n",
       "\n",
       "  stalk_shape_int  stalk_root_int  stalk_surface_above_ring_int  \\\n",
       "0               0               2                             3   \n",
       "1               0               1                             3   \n",
       "2               0               1                             3   \n",
       "3               0               2                             3   \n",
       "4               1               2                             3   \n",
       "\n",
       "   stalk_surface_below_ring_int  veil_color_int  ring_type_int  habitat_int  \n",
       "0                             3               2              4            4  \n",
       "1                             3               2              4            0  \n",
       "2                             3               2              4            2  \n",
       "3                             3               2              4            4  \n",
       "4                             3               2              0            0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_features = [\"cap_shape\", \"cap_surface\", \"bruises\", \n",
    "                     \"gill_attachment\", \"gill_spacing\", \"stalk_shape\", \n",
    "                     \"stalk_root\", \"stalk_surface_above_ring\", \"stalk_surface_below_ring\", \n",
    "                     \"veil_color\", \"ring_number\", \"ring_type\", \"habitat\"]\n",
    "\n",
    "df_new, categorical_headers_ints, numeric_columns, feature_columns = create_new_data_frame(df, included_features, [\"ring_number\"])\n",
    "print(categorical_headers_ints)\n",
    "print(feature_columns)\n",
    "\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model does really well in terms of recall, our evaluation metric, we want to compare models using converence. As in the number of epochs it takes to get to a 100% recall rate. First, we'll try our original model but with only 10 epochs and the features we specified were easily distinguishable to the human eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ERROR ON Defining x using Desnse. Have to tweak some param\n",
    "\n",
    "\n",
    "# %%time\n",
    "# num_folds = 10\n",
    "\n",
    "# X = df_new[feature_columns].values\n",
    "# y = df_new[\"class\"].values\n",
    "\n",
    "# scores = []\n",
    "\n",
    "# new_cross_columns = [['cap_shape','cap_surface'],\n",
    "#          ['gill_attachment', 'gill_spacing'],\n",
    "#          ['stalk_shape', 'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring']]\n",
    "\n",
    "# numeric_headers = ['ring_number']\n",
    "\n",
    "# # print(type(df_new['ring_number'][0]))\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "# for i, (train, test) in enumerate(skf.split(X, y)):\n",
    "#     print(f\"Fold {i}\")\n",
    "    \n",
    "#     X_train = df_new.iloc[train]\n",
    "#     X_test = df_new.iloc[test]\n",
    "#     y_train = df_new[\"class\"].iloc[train].values.astype(np.int)\n",
    "#     y_test = df_new[\"class\"].iloc[test].values.astype(np.int)\n",
    "    \n",
    "#     ohe = OneHotEncoder()\n",
    "#     X_train_ohe = ohe.fit_transform(X_train[categorical_headers_ints].values)\n",
    "#     X_test_ohe = ohe.transform(X_test[categorical_headers_ints].values)\n",
    "\n",
    "#     X_train_num = X_train[numeric_headers].values  # already scaled\n",
    "#     X_test_num = X_test[numeric_headers].values\n",
    "    \n",
    "# #     model = create_model(X_train_ohe, X_train_num)\n",
    "#     model, X_ints_train, X_ints_test, X_train_num = create_model(X_train, X_test, X_train_num, categorical_headers_ints, new_cross_columns)\n",
    "    \n",
    "#     model.compile(optimizer='adagrad',\n",
    "#                   loss='mean_squared_error',\n",
    "#                   metrics=['accuracy'])\n",
    "# #     X_crossed_train, X_crossed_test, model = create_model_wide(X_train, X_test)\n",
    "# #     model.fit([X_train_ohe,X_train_num],y_train, epochs=10, batch_size=50, verbose=0)\n",
    "#     model.fit(X_ints_train + [X_train_num], y_train, epochs=5, batch_size=50, verbose=1)\n",
    "\n",
    "# #     yhat = np.round(model.predict([X_test_ohe,X_test_num]))\n",
    "#     yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "#     print(mt.confusion_matrix(y_test,yhat))\n",
    "#     score = mt.recall_score(y_test, yhat)\n",
    "#     scores.append(score)\n",
    "#     print(\"Recall: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# folds = list(range(0,num_folds))\n",
    "# print(scores)\n",
    "\n",
    "# plt.bar(folds, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we do slightly worse now, in that there are some folds that don't achieve 100% recall. Maybe we can tune a better model to fix that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
